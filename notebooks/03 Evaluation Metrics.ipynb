{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from preamble import *\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['legend.numpoints'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"data/bank-campaign.csv\")\n",
    "X = data.drop(\"target\", axis=1).values\n",
    "y = data.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "print(\"predicted labels: %s\" % np.unique(pred_most_frequent))\n",
    "print(\"score: %f\" % dummy_majority.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dummy = DummyClassifier().fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "print(\"dummy score: %f\" % dummy.score(X_test, y_test))\n",
    "\n",
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "print(\"logreg score: %f\" % logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Most frequent class:\")\n",
    "print(confusion_matrix(y_test, pred_most_frequent))\n",
    "print(\"\\nDummy model:\")\n",
    "print(confusion_matrix(y_test, pred_dummy))\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_test, pred_tree))\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(confusion_matrix(y_test, pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relation to accuracy\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, recall and f-score\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "source": [
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{F} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score most frequent: %.2f\" % f1_score(y_test, pred_most_frequent, pos_label=\"yes\"))\n",
    "print(\"f1 score dummy: %.2f\" % f1_score(y_test, pred_dummy, pos_label=\"yes\"))\n",
    "print(\"f1 score tree: %.2f\" % f1_score(y_test, pred_tree, pos_label=\"yes\"))\n",
    "print(\"f1 score: %.2f\" % f1_score(y_test, pred_logreg, pos_label=\"yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_most_frequent,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_tree,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_logreg,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking uncertainty into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mglearn.datasets import make_blobs \n",
    "from sklearn.svm import SVC\n",
    "X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],        \n",
    "                  random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_decision_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = svc.decision_function(X_test) > -.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curves and ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test,\n",
    "                                                       svc.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a similar dataset as before, but with more samples to get a smoother curve\n",
    "X, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, svc.decision_function(X_test))\n",
    "# find threshold closest to zero:\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.figure()\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(precision, recall, label=\"precision recall curve\")\n",
    "plt.xlabel(\"precision\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.title(\"precision_recall_curve\");\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier has predict_proba, but not decision_function\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_test, rf.predict_proba(X_test)[:, 1])\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(precision, recall, label=\"svc\")\n",
    "\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(precision_rf, recall_rf, label=\"rf\")\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', markersize=10,\n",
    "         label=\"threshold 0.5 rf\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.xlabel(\"precision\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"precision_recall_comparison\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"f1_score of random forest: %f\" % f1_score(y_test, rf.predict(X_test)))\n",
    "print(\"f1_score of svc: %f\" % f1_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
    "print(\"average precision of random forest: %f\" % ap_rf)\n",
    "print(\"average precision of svc: %f\" % ap_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristics (ROC) and AUC\n",
    "\\begin{equation}\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "plt.title(\"roc_curve\");\n",
    "# find threshold closest to zero:\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "plt.title(\"roc_curve_comparison\");\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n",
    "         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print(\"AUC for Random Forest: %f\" % rf_auc)\n",
    "print(\"AUC for SVC: %f\" % svc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop(\"target\", axis=1).values\n",
    "y = data.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, train_size=.1, test_size=.1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for gamma in [1, 0.01, 0.001]:\n",
    "    svc = SVC(gamma=gamma).fit(X_train, y_train)\n",
    "    accuracy = svc.score(X_test, y_test)\n",
    "    auc = roc_auc_score(y_test == \"yes\", svc.decision_function(X_test))\n",
    "    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test), pos_label=\"yes\")\n",
    "    print(\"gamma = %.03f  accuracy = %.02f  AUC = %.02f\" % (gamma, accuracy, auc))\n",
    "    plt.plot(fpr, tpr, label=\"gamma=%.03f\" % gamma, linewidth=4)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.02)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, random_state=0)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_test, pred))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "scores_image = mglearn.tools.heatmap(confusion_matrix(y_test, pred), xlabel='Predicted label', ylabel='True label',\n",
    "                                     xticklabels=digits.target_names, yticklabels=digits.target_names,\n",
    "                                     cmap=plt.cm.gray_r, fmt=\"%d\")    \n",
    "\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"micro average f1 score: %f\" % f1_score(y_test, pred, average=\"micro\"))\n",
    "print(\"macro average f1 score: %f\" % f1_score(y_test, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using evaluation metrics in model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "# default scoring for classification is accuracy\n",
    "print(\"default scoring \", cross_val_score(SVC(), X, y))\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"accuracy\")\n",
    "print(\"explicit accuracy scoring \", explicit_accuracy)\n",
    "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"roc_auc\")\n",
    "print(\"AUC scoring \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# back to the bank campaign\n",
    "X = data.drop(\"target\", axis=1).values\n",
    "y = data.target.values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=.1, test_size=.1, random_state=0)\n",
    "\n",
    "# we provide a somewhat bad grid to illustrate the point:\n",
    "param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "# using the default scoring of accuracy:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Grid-Search with accuracy\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (accuracy)):\", grid.best_score_)\n",
    "print(\"Test set AUC: %.3f\" % roc_auc_score(y_test, grid.decision_function(X_test)))\n",
    "print(\"Test set accuracy %.3f: \" % grid.score(X_test, y_test))\n",
    "\n",
    "# using AUC scoring instead:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"\\nGrid-Search with AUC\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (AUC):\", grid.best_score_)\n",
    "print(\"Test set AUC: %.3f\" % roc_auc_score(y_test, grid.decision_function(X_test)))\n",
    "print(\"Test set accuracy %.3f: \" % grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "    print(sorted(SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_scoring(fitted_estimator, X_test, y_test):\n",
    "    return (fitted_estimator.predict(X_test) == y_test).mean()\n",
    "\n",
    "GridSearchCV(SVC(), param_grid, scoring=my_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Load the adult dataset from ``data/adult.csv``, and split it into training and test set.\n",
    "Apply grid-search to the training set, searching for the best C for Logistic Regression, also search over L1 penalty vs L2 penalty.\n",
    "Plot the ROC curve of the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dummy variables, needed for scikit-learn models on categorical data:\n",
    "X = pd.get_dummies(data.drop(\"income\", axis=1))\n",
    "y = data.income == \" >50K\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
